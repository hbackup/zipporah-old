# this file has location variables used throughout the system

# locations
#==============================================================================
working=/export/a11/hxu/DS/working
ROOT=/export/a11/hxu/DS/data-selection
#==============================================================================

# binaries
#==============================================================================
moses=/export/a11/hxu/mosesdecoder
tagger=/export/a11/hxu/automatic/data-selection/external
clust=/export/a11/hxu/tools/cluster-3.6.7/
kaldi=/export/a11/hxu/kaldi/
srilm=/export/a11/hxu/tools/srilm/bin/i686-m64

gmm_clustering_script=$ROOT/scripts/cluster-gmm.sh
gmm_scoring_script=$ROOT/script/score-gmm.sh

gmm_clustering_script=$ROOT/scripts/cluster-gmm-kaldi.sh
gmm_scoring_script=$ROOT/script/score-gmm-kaldi.sh

#==============================================================================
# corpus
# good corpus - to extract features and train a GMM;
# if it's a number then select a subset of that size from bad corpus

# bad corpus - the corpus which we try to select a good subset
#==============================================================================
input_lang=fr
output_lang=en
#raw_stem_good=2000000
raw_stem_good=/home/pkoehn/statmt/data/wmt15/training/europarl-v7.fr-en

# if good corpus is a subset of bad corpus then in the first iteration we choose
# a random subset and later pick the top-scored ones from the previous iteration
num_iters=4 # TODO not sure if keep this
#clean_stem_good=

clean_stem_bad=/export/a11/hxu/DS/working/1/iter-1/step-1/bad.clean.short

output_words=50000000 # as in number of lines
lang_pair=${input_lang}-${output_lang}

#==============================================================================
# exsiting translation model path
# if this is set, we will not process the *ref corpus
#==============================================================================
f2e=
e2f=

#==============================================================================
# features to use
#==============================================================================
ngram_feat=true
ngram_order=3
pos_ngram_order=5
word_count=10000

bow_feat=false # probably the most important feature
bow_thresh=0.05 # only pick entries in lex table when prob >= thresh
                # 1 means pick most likely translation

pos_feat=true # need tagger
pos_num=5 # only pick top pos_num most common tags for each language
pos_ngram_feat=true
pos_sample=2000 
pos_jobs=80

length_feat=true
length_ratio=true

non_word_agree=true

#==============================================================================
# GMM
#==============================================================================
num_gauss=8
gmm_sample_size=80000

#==============================================================================
# Below here is automatic scripts. Do not change anything
#==============================================================================
if [ "$e2f" != "" ] && [ "$f2e" != "" ]; then
  raw_stem_ref=
  clean_stem_ref=
else
  e2f=$working/$id/iter-$iter/step-2/MT/model/lex.1.e2f
  f2e=$working/$id/iter-$iter/step-2/MT/model/lex.1.f2e
fi

if ! [[ $raw_stem_good =~ ^[0-9]+$ ]]; then
  num_iters=1
fi
  
if [ "$num_gauss" == "" ]; then
  num_gauss=8
fi

set -e
